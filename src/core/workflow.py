# -*- coding: utf-8 -*-

import pandas as pd
from typing import List, Dict

# ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥
from src.data.loader import read_and_validate_excel, apply_formatting_rules, get_source_name
from src.analysis import comparison, historical
from src.llm.client import get_llm_client, request_llm_analysis
from src.llm.prompts import create_comparison_prompt, create_historical_prompt
from src.utils.file_handler import save_text_file, save_markdown_report

def execute_comparison_workflow(analysis_tasks: List[Dict], config: Dict):
    """æ‰§è¡ŒåŒç‰ˆæœ¬é«˜ç²¾åº¦æ¯”å¯¹åˆ†æå·¥ä½œæµã€‚"""
    try:
        # --- é…ç½®æå– ---
        key_columns = config['analysis_params']['key_columns']
        formatting_rules = config['analysis_params']['formatting_rules']
        llm_config = config['llm']
        output_config = config['output']
        log_dir = output_config['log_directory']

        # --- æ•°æ®åŠ è½½ ---
        hist_task, latest_task = analysis_tasks[0], analysis_tasks[1]
        hist_name, latest_name = get_source_name(hist_task), get_source_name(latest_task)

        print(f"  - æ­£åœ¨è¯»å–å†å²ç‰ˆæœ¬: {hist_name}...")
        df_hist_raw = read_and_validate_excel(hist_task['file'], key_columns, hist_task.get('sheet'))
        print(f"  - æ­£åœ¨è¯»å–æœ€æ–°ç‰ˆæœ¬: {latest_name}...")
        df_latest_raw = read_and_validate_excel(latest_task['file'], key_columns, latest_task.get('sheet'))

        # --- æ•°æ®é¢„å¤„ç† ---
        print("  - æ­£åœ¨åº”ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–è§„åˆ™...")
        df_hist = apply_formatting_rules(df_hist_raw, formatting_rules)
        df_latest = apply_formatting_rules(df_latest_raw, formatting_rules)

        # --- æ ¸å¿ƒåˆ†æ ---
        print("  - æ­£åœ¨ç”Ÿæˆå·®å¼‚æŠ¥å‘Š...")
        diff_report = comparison.generate_precise_diff_report(df_hist, df_latest, key_columns, hist_name, latest_name)

        # --- LLM äº¤äº’ ---
        prompt = create_comparison_prompt(diff_report, hist_name, latest_name)
        print("\nğŸ“ æ­£åœ¨ç”Ÿæˆå¯¹æ¯”åˆ†æPrompt...")
        save_text_file(f'{log_dir}/prompts', 'precise_comparison_prompt', prompt)

        print("ğŸ¤– æ­£åœ¨è¯·æ±‚å¤§æ¨¡å‹è¿›è¡Œåˆ†æ...")
        llm_client = get_llm_client()
        assistant_response = request_llm_analysis(
            client=llm_client,
            model=llm_config['model_name'],
            system_prompt=llm_config['system_prompts']['comparison'],
            user_prompt=prompt
        )
        save_text_file(f'{log_dir}/results', 'precise_comparison_result', assistant_response)

        # --- ç»“æœä¿å­˜ ---
        print(f"\nâœ… **æœ€ç»ˆç»¼åˆåˆ†ææŠ¥å‘Š**:\n{assistant_response}")
        save_markdown_report(output_config['report_directory'], 'Precise_Comparison_Report', assistant_response)

    except Exception as e:
        print(f"âŒ åœ¨æ‰§è¡Œå¯¹æ¯”åˆ†æå·¥ä½œæµæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def execute_historical_workflow(analysis_tasks: List[Dict], config: Dict):
    """æ‰§è¡Œå¤šç‰ˆæœ¬å†å²è¿½æº¯åˆ†æå·¥ä½œæµã€‚"""
    try:
        # --- é…ç½®æå– ---
        params = config['analysis_params']
        key_columns, value_column = params['key_columns'], params['value_column']
        formatting_rules, top_n = params['formatting_rules'], params['top_n_for_analysis']
        llm_config = config['llm']
        output_config = config['output']
        log_dir = output_config['log_directory']

        # --- æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ---
        all_dfs = []
        for i, task in enumerate(analysis_tasks):
            source_name = get_source_name(task)
            print(f"  - æ­£åœ¨è¯»å–ç‰ˆæœ¬ {i + 1}: {source_name}")
            df_raw = read_and_validate_excel(task['file'], key_columns, task.get('sheet'))
            df_formatted = apply_formatting_rules(df_raw, formatting_rules)
            all_dfs.append(df_formatted)

        # --- æ ¸å¿ƒåˆ†æ ---
        trace_df = historical.generate_historical_trace_table(all_dfs, key_columns, value_column, top_n)
        md_trace_table = historical.create_historical_trace_markdown(trace_df, key_columns)

        # ä¸ºæ‘˜è¦å‡†å¤‡é¦–å°¾ç‰ˆæœ¬å¯¹æ¯”
        df_first = all_dfs[0]
        df_last = all_dfs[-1]
        summary_diff_report = comparison.generate_precise_diff_report(df_first, df_last, key_columns, columns_to_check=[value_column])
        summary_line = summary_diff_report.split('\n')[0]
        source_names = " -> ".join([get_source_name(task) for task in analysis_tasks])

        # --- LLM äº¤äº’ ---
        prompt = create_historical_prompt(
            md_trace_table, summary_line, source_names, value_column, len(trace_df)
        )
        print("\nğŸ“ æ­£åœ¨ç”Ÿæˆå†å²è¿½æº¯Prompt...")
        save_text_file(f'{log_dir}/prompts', 'historical_trace_prompt', prompt)

        print("ğŸ¤– æ­£åœ¨è¯·æ±‚å¤§æ¨¡å‹è¿›è¡Œåˆ†æ...")
        llm_client = get_llm_client()
        assistant_response = request_llm_analysis(
            client=llm_client,
            model=llm_config['model_name'],
            system_prompt=llm_config['system_prompts']['historical'],
            user_prompt=prompt
        )
        save_text_file(f'{log_dir}/results', 'historical_trace_result', assistant_response)

        # --- ç»“æœä¿å­˜ ---
        print(f"\nâœ… **æœ€ç»ˆç»¼åˆåˆ†ææŠ¥å‘Š**:\n{assistant_response}")
        save_markdown_report(output_config['report_directory'], 'Historical_Trace_Report', assistant_response)

    except Exception as e:
        print(f"âŒ åœ¨æ‰§è¡Œå†å²è¿½æº¯å·¥ä½œæµæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()